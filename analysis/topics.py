# analysis/topics.py (Enhanced)
"""
Enhanced topic analysis with sentiment integration and better categorization
"""

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

from config import ANALYSIS_CONFIG
from collections import defaultdict


class TopicAnalyzer:
    def __init__(self, openai_api_key=None):
        self.openai_client = OpenAI(api_key=openai_api_key) if openai_api_key and OPENAI_AVAILABLE else None
        self.bulk_topics = []
        self.topic_cache = {}
        self.total_tokens_used = 0
        self.topic_sentiment_map = {}  # Store topic-sentiment mapping
    
    def generate_bulk_topic_analysis_with_sentiment(self, tweets_sample, token_symbol):
        """Generate bulk topic analysis with sentiment - enhanced categorization"""
        if self.bulk_topics:  # Skip if already done
            return self.bulk_topics
        
        if not self.openai_client:
            print("   ğŸ’¡ OpenAIä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…è·å–å…·ä½“è¯é¢˜...")
            self.bulk_topics = self._extract_fallback_topics_with_sentiment(tweets_sample)
            return self.bulk_topics
        
        try:
            # Limit to avoid token limits
            max_tweets = ANALYSIS_CONFIG['max_tweets_for_topic_analysis']
            sample_tweets = tweets_sample[:max_tweets] if len(tweets_sample) > max_tweets else tweets_sample
            tweets_text = "\n".join([f"{i+1}. {tweet['text'][:150]}..." if len(tweet['text']) > 150 else f"{i+1}. {tweet['text']}" 
                                   for i, tweet in enumerate(sample_tweets)])
            
            prompt = f"""
            è¯·åˆ†æè¿™{len(sample_tweets)}æ¡å…³äº{token_symbol}çš„æ¨æ–‡ï¼Œè¯†åˆ«ä¸»è¦è®¨è®ºè¯é¢˜å¹¶å½’ç±»å…¶æƒ…æ„Ÿå€¾å‘ã€‚è¯·ç”¨ç®€ä½“ä¸­æ–‡å›å¤ã€‚
            
            æ¨æ–‡å†…å®¹:
            {tweets_text}
            
            é‡è¦è¯´æ˜ï¼š
            1. åˆå¹¶ç›¸ä¼¼çš„ä»·æ ¼/äº¤æ˜“ç›¸å…³è¯é¢˜ï¼Œé¿å…è¿‡åº¦ç»†åˆ†
            2. ä¸ºæ¯ä¸ªè¯é¢˜æ·»åŠ æ˜ç¡®çš„æƒ…æ„Ÿæ–¹å‘
            3. ä¼˜å…ˆè¯†åˆ«å…·æœ‰æ˜ç¡®æƒ…æ„Ÿçš„é‡è¦è¯é¢˜
            
            è¯é¢˜åˆ†ç±»æŒ‡å¼•ï¼ˆå¸¦æƒ…æ„Ÿï¼‰ï¼š
            
            ä»·æ ¼äº¤æ˜“ç±»ï¼ˆåˆå¹¶å¤„ç†ï¼‰:
            - ä»·æ ¼çœ‹æ¶¨ (ä»·æ ¼é¢„æµ‹/æŠ€æœ¯åˆ†æ/çªç ´ä¿¡å·ç­‰çœ‹æ¶¨å†…å®¹)
            - ä»·æ ¼çœ‹è·Œ (ä»·æ ¼é¢„æµ‹/æŠ€æœ¯åˆ†æ/çªç ´ä¿¡å·ç­‰çœ‹è·Œå†…å®¹)
            - äº¤æ˜“åˆ†äº« (æŒä»“åˆ†äº«/äº¤æ˜“ç­–ç•¥/ä¹°å–æ“ä½œç­‰ï¼Œæ ‡æ³¨çœ‹æ¶¨/çœ‹è·Œ)
            
            é¡¹ç›®å‘å±•ç±»:
            - åˆ©å¥½æ¶ˆæ¯ (ä¸Šå¸/åˆä½œ/äº§å“å‘å¸ƒç­‰ç§¯ææ¶ˆæ¯)
            - åˆ©ç©ºæ¶ˆæ¯ (ä¸‹æ¶/ç›‘ç®¡/æŠ€æœ¯é—®é¢˜ç­‰æ¶ˆææ¶ˆæ¯)
            
            ç¤¾åŒºæƒ…ç»ªç±»:
            - ç¤¾åŒºä¹è§‚ (ç§¯æè®¨è®º/çœ‹å¥½æœªæ¥)
            - ç¤¾åŒºæ‹…å¿§ (é£é™©è­¦å‘Š/è´Ÿé¢æƒ…ç»ª)
            
            æŠ€æœ¯é£é™©ç±»:
            - å®‰å…¨é£é™© (é»‘å®¢/æ¼æ´ç­‰)
            - åˆè§„é£é™© (ç›‘ç®¡/æ³•å¾‹é—®é¢˜)
            
            è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆæœ€å¤š6ä¸ªä¸»è¦è¯é¢˜ï¼‰ï¼š
            
            è¯é¢˜1: [è¯é¢˜åç§°+æƒ…æ„Ÿ] - [æ¨æ–‡ç¼–å·,æ¨æ–‡ç¼–å·,...]
            è¯é¢˜2: [è¯é¢˜åç§°+æƒ…æ„Ÿ] - [æ¨æ–‡ç¼–å·,æ¨æ–‡ç¼–å·,...]
            
            è¦æ±‚ï¼š
            1. è¯é¢˜åç§°è¦å…·ä½“ä¸”åŒ…å«æƒ…æ„Ÿæ–¹å‘ï¼Œå¦‚"ä»·æ ¼çœ‹æ¶¨"ã€"äº¤æ˜“åˆ†äº«-çœ‹æ¶¨"ã€"åˆ©å¥½æ¶ˆæ¯"
            2. é¿å…è¿‡åº¦ç»†åˆ†ç›¸ä¼¼è¯é¢˜
            3. æ¯ä¸ªè¯é¢˜è‡³å°‘åŒ…å«2æ¡æ¨æ–‡
            4. æŒ‰è®¨è®ºçƒ­åº¦æ’åº
            5. è¯é¢˜åç§°ä¸è¶…è¿‡8ä¸ªå­—
            
            ç¤ºä¾‹æ ¼å¼:
            è¯é¢˜1: ä»·æ ¼çœ‹æ¶¨ - 1,3,5,7
            è¯é¢˜2: äº¤æ˜“åˆ†äº«-çœ‹æ¶¨ - 2,4,6
            è¯é¢˜3: åˆ©å¥½æ¶ˆæ¯ - 8,9
            """
            
            response = self.openai_client.chat.completions.create(
                model=ANALYSIS_CONFIG['openai_model'],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.2
            )
            
            # Track token usage
            if hasattr(response, 'usage'):
                self.total_tokens_used += response.usage.total_tokens
            
            # Parse the response
            content = response.choices[0].message.content.strip()
            topics = []
            
            for line in content.split('\n'):
                line = line.strip()
                if line and 'è¯é¢˜' in line and ':' in line and '-' in line:
                    try:
                        # Extract topic name and tweet numbers
                        parts = line.split(':', 1)[1].strip().split(' - ')
                        if len(parts) == 2:
                            topic_name = parts[0].strip()
                            tweet_numbers_str = parts[1].strip()
                            
                            # Clean up topic name
                            if topic_name.startswith('- '):
                                topic_name = topic_name[2:].strip()
                            if topic_name.startswith('-'):
                                topic_name = topic_name[1:].strip()
                            if topic_name.startswith('[') and topic_name.endswith(']'):
                                topic_name = topic_name[1:-1].strip()
                            
                            # Only include if reasonably specific and has sentiment
                            if len(topic_name) <= 12:  # Slightly longer to accommodate sentiment
                                # Count tweets for this topic
                                tweet_count = len([x.strip() for x in tweet_numbers_str.split(',') if x.strip()])
                                
                                if tweet_count >= 1:  # At least 1 tweet
                                    topics.append({
                                        'name': topic_name,
                                        'count': tweet_count,
                                        'tweet_numbers': tweet_numbers_str
                                    })
                    except:
                        continue
            
            # If we still don't have enough topics, add fallback
            if len(topics) < 3:
                print("   ğŸ’¡ AIè¿”å›è¯é¢˜ä¸è¶³ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…è¡¥å……...")
                fallback_topics = self._extract_fallback_topics_with_sentiment(tweets_sample)
                existing_names = {t['name'] for t in topics}
                for fallback_topic in fallback_topics:
                    if fallback_topic['name'] not in existing_names and len(topics) < 6:
                        topics.append(fallback_topic)
            
            # Sort by count and return top topics
            self.bulk_topics = sorted(topics, key=lambda x: x['count'], reverse=True)[:6]
            return self.bulk_topics
            
        except Exception as e:
            print(f"OpenAIè¯é¢˜åˆ†æé”™è¯¯: {e}")
            self.bulk_topics = self._extract_fallback_topics_with_sentiment(tweets_sample)
            return self.bulk_topics
    
    def _extract_fallback_topics_with_sentiment(self, tweets_sample):
        """Extract topics with sentiment using keyword matching as fallback"""
        topic_keywords = {
            'ä»·æ ¼çœ‹æ¶¨': ['moon', 'pump', 'bullish', 'çœ‹æ¶¨', 'ä¸Šæ¶¨', 'çªç ´', 'ç‰›å¸‚', 'ç›®æ ‡ä»·', 'target'],
            'ä»·æ ¼çœ‹è·Œ': ['dump', 'bearish', 'çœ‹è·Œ', 'ä¸‹è·Œ', 'å´©ç›˜', 'ç†Šå¸‚', 'æŠ›å”®', 'crash'],
            'äº¤æ˜“åˆ†äº«-çœ‹æ¶¨': ['ä¹°å…¥', 'buy', 'long', 'åŠ ä»“', 'å»ºä»“', 'æŒæœ‰', 'hold'],
            'äº¤æ˜“åˆ†äº«-çœ‹è·Œ': ['å–å‡º', 'sell', 'short', 'å‡ä»“', 'æ­¢æŸ', 'å‡ºè´§'],
            'åˆ©å¥½æ¶ˆæ¯': ['ä¸Šå¸', 'ä¸Šæ‰€', 'åˆä½œ', 'æ›´æ–°', 'å‘å¸ƒ', 'listing', 'partnership'],
            'åˆ©ç©ºæ¶ˆæ¯': ['ä¸‹æ¶', 'æš‚åœ', 'delist', 'é£é™©', 'è­¦å‘Š', 'risk'],
            'ç¤¾åŒºä¹è§‚': ['çœ‹å¥½', 'æ½œåŠ›', 'æœºä¼š', 'æ¨è', 'gem', 'alpha'],
            'ç¤¾åŒºæ‹…å¿§': ['æ‹…å¿ƒ', 'é£é™©', 'å°å¿ƒ', 'è°¨æ…', 'risky', 'caution'],
            'æŠ€æœ¯åˆ†æ': ['æŠ€æœ¯', 'å›¾è¡¨', 'åˆ†æ', 'TA', 'chart', 'æ”¯æ’‘', 'é˜»åŠ›'],
            'ç©ºæŠ•ç¦åˆ©': ['ç©ºæŠ•', 'airdrop', 'å…è´¹', 'ç¦åˆ©', 'èµ é€']
        }
        
        fallback_topics = []
        
        for topic, keywords in topic_keywords.items():
            count = 0
            tweet_numbers = []
            
            for i, tweet in enumerate(tweets_sample[:15]):
                tweet_text = tweet.get('text', '').lower()
                if any(keyword.lower() in tweet_text for keyword in keywords):
                    count += 1
                    tweet_numbers.append(str(i + 1))
            
            if count > 0:
                fallback_topics.append({
                    'name': topic,
                    'count': count,
                    'tweet_numbers': ','.join(tweet_numbers[:min(count, 10)])
                })
        
        return fallback_topics
    
    def get_tweet_topic_with_sentiment(self, tweet_text, combined_result=None):
        """Get specific topic with sentiment from combined analysis"""
        # First try to use combined analysis result
        if combined_result and combined_result.get('topic'):
            topic = combined_result['topic']
            if topic.startswith('- '):
                topic = topic[2:].strip()
            if topic.startswith('-'):
                topic = topic[1:].strip()
            
            # Check if it already has sentiment or try to add sentiment
            if topic and topic != "æœªåˆ†ç±»" and len(topic) <= 12:
                # If topic doesn't have sentiment, try to determine it
                if not any(sentiment_word in topic for sentiment_word in ['çœ‹æ¶¨', 'çœ‹è·Œ', 'ä¹è§‚', 'æ‹…å¿§', 'åˆ©å¥½', 'åˆ©ç©º']):
                    return self._add_sentiment_to_topic(topic, tweet_text, combined_result)
                return topic
        
        # Fallback to bulk topic matching
        if self.bulk_topics:
            tweet_lower = tweet_text.lower()
            
            # Try to match with existing bulk topics (which should have sentiment)
            for topic in self.bulk_topics:
                topic_name = topic['name'].lower()
                # Extract keywords from topic name for matching
                topic_keywords = topic_name.replace('-', ' ').split()
                if any(keyword in tweet_lower for keyword in topic_keywords if len(keyword) > 2):
                    return topic['name']
        
        return "æœªåˆ†ç±»"
    
    def _add_sentiment_to_topic(self, base_topic, tweet_text, combined_result):
        """Add sentiment direction to a base topic"""
        # Determine sentiment from combined result or tweet text
        sentiment = 'NEUTRAL'
        if combined_result:
            sentiment = combined_result.get('sentiment', 'NEUTRAL')
        
        # Map base topics to sentiment-aware versions
        if base_topic in ['ä»·æ ¼é¢„æµ‹', 'æŠ€æœ¯åˆ†æ', 'çªç ´ä¿¡å·']:
            if sentiment == 'POSITIVE':
                return 'ä»·æ ¼çœ‹æ¶¨'
            elif sentiment == 'NEGATIVE':
                return 'ä»·æ ¼çœ‹è·Œ'
            else:
                return 'æŠ€æœ¯åˆ†æ'
        elif base_topic in ['æŒä»“åˆ†äº«', 'äº¤æ˜“ç­–ç•¥']:
            if sentiment == 'POSITIVE':
                return 'äº¤æ˜“åˆ†äº«-çœ‹æ¶¨'
            elif sentiment == 'NEGATIVE':
                return 'äº¤æ˜“åˆ†äº«-çœ‹è·Œ'
            else:
                return 'äº¤æ˜“åˆ†äº«'
        elif base_topic in ['ä¸Šå¸', 'åˆä½œä¼™ä¼´', 'äº§å“å¼€å‘']:
            return 'åˆ©å¥½æ¶ˆæ¯'
        elif base_topic in ['rug pull', 'ä¸‹æ¶', 'é£é™©æç¤º']:
            return 'åˆ©ç©ºæ¶ˆæ¯'
        else:
            return base_topic
    
    def analyze_topic_sentiment_distribution(self, tweet_analyses):
        """Analyze sentiment distribution for each topic"""
        topic_sentiment_counts = defaultdict(lambda: {'POSITIVE': 0, 'NEGATIVE': 0, 'NEUTRAL': 0, 'total': 0})
        
        for tweet in tweet_analyses:
            topic = tweet.get('topic', 'æœªåˆ†ç±»')
            sentiment = tweet['sentiment']['sentiment']
            
            topic_sentiment_counts[topic][sentiment] += 1
            topic_sentiment_counts[topic]['total'] += 1
        
        return dict(topic_sentiment_counts)